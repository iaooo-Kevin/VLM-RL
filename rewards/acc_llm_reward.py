"""
Use this open-ended framework where there is a case of tedious string-matching which can
make or break the reward function. Oftentimes, the model's output is not exactly the same as the ground truth, albeit being semantically equivalent.
LLM-as-a-Judge is an attempt to bypass this brittle nature of string matching and avoiding the rigamarole of setting up dataset specific heuristics.
"""
from typing import List
import os
from openai import OpenAI
import re
#Load the .env file
from dotenv import load_dotenv
load_dotenv()
apiKey = os.getenv("OPENAI_API_KEY")

def judgeToReward(score: float) -> float:
    """
    Convert the judge score (0-1) to a reward (-1-1)
    Args:
        score: The score given by the judge (0-1)
    Returns:
        Reward (-1 to 1)
    """
    if score >= 0.85:
        return 1.0
    elif score >= 0.7:
        return 0.7
    elif score >= 0.55:
        return 0.5
    elif score >= 0.4:
        return 0.0
    elif score >= 0.25:
        return -0.5
    else:
        return -1.0


def AccuracyLLM(completions, solution, **kwargs) -> List[float]:
    """
    Reward function that checks and evaluates the reasoning process leading to the answer by the Policy LLM.
    Using LLM-as-a-judge framework to emit a reward between 0 and 1.
    Use this open-ended framework where there is a case of tedious string-matching which can
    make or break the reward function. Oftentimes, the model's output is not exactly the same as the ground truth, albeit being semantically equivalent.
    LLM-as-a-Judge is an attempt to bypass this brittle nature of string matching and avoiding the rigamarole of setting up dataset specific heuristics.

    Granted, it is not perfect and his its own faults, but it is a good starting point for evaluating reasoning processes and / or accuracy of answers.
    Arguments:
        completions: The completions generated by the model.
        solution: The ground truth solution.
        **kwargs: Other key word arguments.
    Returns:
        Reward vector (List[float])
    """
    if not apiKey:
        print(f'Warning, no API key found in the environment. Reasoning rewards will be disabled.')
        return [0.0] * len(completions)
    JudgeTemplate = """
    You are an expert evaluator. Grade whether the Student's answer matches the Golden answer.
    Rules:
    1). Evaluate the critical / most relevant parts only (ignore grammar, punctuation, etc.)
    2). If Medical answers are involved, accept standard synonyms (e.g. hypertension = high blood pressure, “acute appendicitis” ~ “blind-ending tubular structure arising from the cecum”)
    3). If the golden answer is "normal" or "healthy", then accept any answer that indicates no abnormalities. in this case `abnormalities detected` by the student should be marked wrong
    4). Extra non-contradictory detail is acceptable; contradictory extra findings make it incorrect
    5). Empty or non-diagnostic answers are incorrect
    6). Discourage ambiguity. If the student is unsure, but the golden answer is certain, mark it wrong.
    7). If the student answer is partially correct, give partial credit.

    Scoring Rubric:
    (In case of medical answers):
    - 1.0  → Correct / semantically equivalent
    - 0.75 → Mostly correct (same entity; minor modifier mismatch)
    - 0.5  → Partially correct (right organ/structure; wrong specific abnormality)
    - 0.25 → Vaguely related (same region/system; wrong concept)
    - 0.0  → Incorrect / contradictory / missing

    Here is the student's answer and the golden answer:
    Golden answer: {golden_cot}
    Student's answer: {studentThoughts}
    """
    client = OpenAI(api_key=apiKey)
    rewards = []
    for completion, golden in zip(completions, solution):
        studentThoughts = re.search(r'<answer>(.*?)</answer>', completion[0]["content"], re.DOTALL)
        studentCoT = studentThoughts.group(1).strip() if studentThoughts else ""
        if not studentCoT:
            rewards.append(0.0)
            continue
        prompt = JudgeTemplate.format(golden_cot=golden, studentThoughts=studentCoT)
        try:
            response = client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[
                    {"role": "system", "content": "You are an expert evaluator of student answers and comparing them to golden answers."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=60
            )
            rawScore = response.choices[0].message.content
            cleanScore = re.search(r'\d+(\.\d+)?', rawScore)
            if cleanScore:
                judgeScore = float(cleanScore.group(0))
                reward = judgeToReward(judgeScore)
                #Clamped reward between -1 and 1
                # print(f'Judge score: {judgeScore}, converted reward: {reward}')
                # reward = float(cleanScore.group(0))
                # print(f'Accuracy reward generated: {reward}')
            else:
                reward = 0.0
        except Exception as e:
            print(f"Error generating accuracy reward: {e}")
            reward = 0.0
        rewards.append(reward)
    return rewards
