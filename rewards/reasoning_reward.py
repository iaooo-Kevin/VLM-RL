from openai import OpenAI
from dotenv import load_dotenv
import os
import logging
import re
from typing import List, Dict, Any
#Load the .env file
load_dotenv()
apiKey = os.getenv("OPENAI_API_KEY")

logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)
def reasoningReward(completions, golden_cot, **kwargs):
    """
    Reward function that checks and evaluates the reasoning process leading to the answer by the Policy LLM. 
    Using LLM-as-a-judge framework to emit a reward between 0 and 1.
    """
    print('We are inside the reasoning reward function')
    if not apiKey:
        print(f'Warning, no API key found in the environment. Reasoning rewards will be disabled.')
        return [0.0] * len(completions)
    JudgeTemplate = """
    You are an expert evaluator of reasoning processes generated by Student models. Your task is to compare the student's reasoning against a golden,
    expert provided reasoning chain. Your evaluation should be strictly a score from 0 to 1, where 1 is the best and 0 is the worst.
    You should not provide any explanation or reasoning for your score. The core idea is to evaluate the student's reasoning process - and not the correctness of the answer. Bear this in mind when evaluating the student's reasoning.
    
    Here is the golden reasoning chain:
    {golden_cot}

    Here is the student's reasoning chain:
    {studentThoughts}

    Please provide a score for the student's reasoning process in the range of [0, 1].
    """
    client = OpenAI(api_key=apiKey)
    rewards = []

    for completion, golden in zip(completions, golden_cot):
        studentThoughts = re.search(r'<think>(.*?)</think>', completion[0]["content"], re.DOTALL)
        studentCoT = studentThoughts.group(1).strip() if studentThoughts else ""
        if not studentCoT:
            rewards.append(0.0)
            continue
        prompt = JudgeTemplate.format(golden_cot=golden, studentThoughts=studentCoT)
        try:
            response = client.chat.completions.create(
                model = 'gpt-4o-mini',
                messages = [
                    {"role": "system", "content": "You are an expert evaluator of reasoning processes."},
                    {"role": "user", "content": prompt}
                ],
                temperature = 0.0,
                max_tokens = 15
            )
            rawScore = response.choices[0].message.content
            cleanScore = re.search(r'\d+(\.\d+)?', rawScore)
            if cleanScore:
                reward = float(cleanScore.group(1))
                print(f'Reward generated: {reward}')
            else:
                reward = 0.0
        except Exception as e:
            print(f"Error generating reasoning reward: {e}")
            reward = 0.0
        rewards.append(reward)
    return rewards

def reasoning_reward(completions: List[Dict[str, Any]], question: List[str], **kwargs) -> List[float]:
    """
    Use this reward function when there are no golden chains of thoughts available, and you would like to award the student model's reasoning steps,
    Uses the LLM-as-a-Judge framework to emit unbounded rewards.
    Args:
        completions: List[Dict[str, Any]]
    Returns
        List[float]: The reward vector
    """
    if not apiKey:
        logger.info('[WARNING] No API key found. In order to use this as a reward function you must provide valid API key')
        return len(completions) * [0.0]
    client = OpenAI(api_key=apiKey)
    rewards = []
    contents = [completion[0]['content'] for completion in completions]
    template = """You are an expert evaluator a student model's reasoning process. You are not supposed to evaluate the model's correctness, rather the quality of its thoughts generated to arrive at the final answer.
    You will have access to both the student's thinking process, and the original question asked, the goal is for you to, again not evaluate correctness of a model's response, but at the same time, don't let a model gaslight itself into providing what it thinks is the correct answer.
    The rewards are to be disbursed in the range: [0, 1]. No additional explanation about the evaluation score is to be given, you are just to give the reward, that's it.

    Here is the question:
    {question}

    Here is the model's thinking process to arrive at a solution:
    {reasonining}
    """
    for content, prob in zip(contents, question):
        chainOfThought = re.search(r'<think>.*?</think>\s', content, re.DOTALL)
        reasoning = chainOfThought.group(1).strip() if chainOfThought else ""
        if not reasoning:
            rewards.append(0.0)
            continue
        prompt = template.format(question=prob, reasoning=reasoning)
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages={"role": "user", "content": prompt},
                temperature=1.0,
                max_tokens=60
            )
            rawScore = response.choices[0].message.content
            cleanScore = re.search(r'\d+(\.\d+)?', rawScore)
            if cleanScore:
                reward = float(cleanScore.group(1))
            else:
                reward = 0.0
        except Exception as e:
            logger.info(f'Error generating reward: {str(e)}')
            reward = 0.0
        rewards.append(reward)
    
    return rewards