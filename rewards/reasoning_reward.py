from openai import OpenAI 
from dotenv import load_dotenv
import os
import re
#Load the .env file
load_dotenv()
apiKey = os.getenv("OPENAI_API_KEY")

def reasoningReward(completions, golden_cot, **kwargs):
    """
    Reward function that checks and evaluates the reasoning process leading to the answer by the Policy LLM. 
    Using LLM-as-a-judge framework to emit a reward between 0 and 1.
    """
    print('We are inside the reasoning reward function')
    if not apiKey:
        print(f'Warning, no API key found in the environment. Reasoning rewards will be disabled.')
        return [0.0] * len(completions)
    JudgeTemplate = """
    You are an expert evaluator of reasoning processes generated by Student models. Your task is to compare the student's reasoning against a golden,
    expert provided reasoning chain. Your evaluation should be strictly a score from 0 to 1, where 1 is the best and 0 is the worst.
    You should not provide any explanation or reasoning for your score. The core idea is to evaluate the student's reasoning process - and not the correctness of the answer. Bear this in mind when evaluating the student's reasoning.
    
    Here is the golden reasoning chain:
    {golden_cot}

    Here is the student's reasoning chain:
    {studentThoughts}

    Please provide a score for the student's reasoning process in the range of [0, 1].
    """
    client = OpenAI(api_key=apiKey)
    rewards = []

    for completion, golden in zip(completions, golden_cot):
        studentThoughts = re.search(r'<think>(.*?)</think>', completion[0]["content"], re.DOTALL)
        studentCoT = studentThoughts.group(1).strip() if studentThoughts else ""
        if not studentCoT:
            rewards.append(0.0)
            continue
        prompt = JudgeTemplate.format(golden_cot=golden, studentThoughts=studentCoT)
        try:
            response = client.chat.completions.create(
                model = 'gpt-4o-mini',
                messages = [
                    {"role": "system", "content": "You are an expert evaluator of reasoning processes."},
                    {"role": "user", "content": prompt}
                ],
                temperature = 0.0,
                max_tokens = 10
            )
            rawScore = response.choices[0].message.content
            cleanScore = re.search(r'\d+(\.\d+)?', rawScore)
            if cleanScore:
                reward = float(cleanScore.group(1))
                print(f'Reward generated: {reward}')
            else:
                reward = 0.0
        except Exception as e:
            print(f"Error generating reasoning reward: {e}")
            reward = 0.0
        rewards.append(reward)
    return rewards